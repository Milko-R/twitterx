# -*- coding: utf-8 -*-
"""Copy of Trabajo grupal Programación (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UUPAw2_dNiBJZFlhQ-fqVj8QaS24cJo4

# Tarea: Tweeter Sentiment Analysis (Fecha de Entrega antes de la siguiente clase)

1. Preprocesar cada uno de los tweets usando las técnicas que vimos en la clase (8 puntos).
2. Visualizar una nube de palabras para los tweets marcados como positivos y como negativos. (2 puntos)
3. Usando la columna label, crear un modelo clasificador de tweets. Probar con BOW, TFIDF y Word2Vec. Mostrar los resultados de accuracy para cada modelo (5 puntos)
4. Seleccionar aleatoriamente 100 tweets. Luego crear una función para que recorra cada uno de los 100 tweets y clasfique como Positivo o Negativo usando el API de ChatGPT. Comparar este resultado con los resultados usando los modelos de la parte 3. ¿A cuál más se parecen los resultados? Comentar. (5 puntos)
"""

import pandas as pd

tweets_df = pd.read_csv('https://raw.githubusercontent.com/CristhianCastro25/Programacion_Analisis_Exploratorio/main/twitter.csv')

tweets_df

tweets_df['label'].value_counts()

tweets_df[tweets_df['label']==1]['tweet'].iloc[10]

tweets_df[tweets_df['label']==0]['tweet'].iloc[1]

texto = tweets_df[tweets_df['label']==1]['tweet'].iloc[300]
texto

texto = tweets_df[tweets_df['label']==1]['tweet'].iloc[550]
texto

"""<h5>Eliminamos palabras con caracteres raros y nos quedamos con palabras que usen letras</h5>"""

import pandas as pd
import re


def get_unique_words(df):

    unique_words = set()
    for tweet in df['tweet']:
        # Tokenize and clean words
        words = re.findall(r'\b[a-zA-ZñÑáéíóúÁÉÍÓÚüÜ]{3,}\b', tweet.lower())
        for word in words:
            unique_words.add(word)
    return list(unique_words)

unique_words_list = get_unique_words(tweets_df)
print(f"Cantidad de palabras únicas: {len(unique_words_list)}")
unique_words_list.sort()
print(unique_words_list)

"""<h4>1.Preprocesar cada uno de los tweets usando las técnicas que vimos en la clase (8 puntos).</h4>

## Stemming
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import pandas as pd

# Aplicar stemming a cada palabra en la lista
porter = nltk.PorterStemmer()
stemmed_words = [porter.stem(word) for word in unique_words_list]

# Imprimir los resultados
for original, stemmed in zip(unique_words_list, stemmed_words):
    print(f"{original} -> {stemmed}")

"""## Lemmatization"""

lemmatizer = nltk.WordNetLemmatizer()
nltk.download('wordnet')

lemmatized_words = [lemmatizer.lemmatize(word) for word in unique_words_list]

print("\nResultados de Lemmatization:")
for original, lemmatized in zip(unique_words_list, lemmatized_words):
    print(f"{original} -> {lemmatized}")



"""## Preprocesamiento completo (lematización, tokenización, stopwords), se omite Stemming porque se usa Lematización

"""

import re
import nltk
nltk.download('punkt') #
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

stop_words = stopwords.words('english') # Definimos stop words en Inglés
stop_words

stop_words.remove('not') # No remover estp de stop words, podría indicar la presencia de algo negativo en un tweet

def data_preprocessing(review):

  # Data cleaning
  review = re.sub(re.compile('<.*?>'), '', review) #removiendo tags html
  review =  re.sub('[^A-Za-z0-9]+', ' ', review) #solo considerar palabras

  # Normalización todo a minúscula
  review = review.lower()

  # Tokenization
  tokens = nltk.word_tokenize(review)

  # Removemos stop_words
  review = [word for word in tokens if word not in stop_words]

  # Lemmatization
  review = [lemmatizer.lemmatize(word) for word in review]

  # Unimos las palabras procesadas
  review = ' '.join(review)

  return review

tweets_df['preprocessed_tweet'] = tweets_df['tweet'].apply(lambda review: data_preprocessing(review))
tweets_df.head()

"""<h2>Visualizar una nube de palabras para los tweets marcados como positivos y como negativos. (2 puntos)</h2>"""

!pip install WordCloud
from wordcloud import WordCloud

"""<h5>Negativos</h5>"""

import matplotlib.pyplot as plt
negative = tweets_df[tweets_df['label']==1]
negative_list = negative['tweet'].tolist()
negative_list
negative_sentences_as_one_string = " ".join(negative_list)
plt.figure(figsize=(10,10))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))

"""<h5>Positivos</h5>"""

positive = tweets_df[tweets_df['label']==0]
positive_list = positive['tweet'].tolist()
positive_list
pos_sentences_as_one_string = " ".join(positive_list)
plt.figure(figsize=(10,10))
plt.imshow(WordCloud().generate(pos_sentences_as_one_string))